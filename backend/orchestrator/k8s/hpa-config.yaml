---
# Horizontal Pod Autoscaler configuration for Agent Orchestrator
# This configuration enables dynamic scaling based on pending tasks per agent

apiVersion: v1
kind: Namespace
metadata:
  name: agent-orchestrator
---
# ServiceMonitor for Prometheus to scrape metrics
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: agent-orchestrator-metrics
  namespace: agent-orchestrator
spec:
  selector:
    matchLabels:
      app: agent-orchestrator
  endpoints:
  - port: metrics
    interval: 15s
    path: /metrics
---
# HPA for Planner Agents
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: planner-agent-hpa
  namespace: agent-orchestrator
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: planner-agent
  minReplicas: 2
  maxReplicas: 100
  metrics:
  # Scale based on pending tasks per agent
  - type: External
    external:
      metric:
        name: pending_tasks_per_agent
        selector:
          matchLabels:
            agent_type: "planner"
      target:
        type: AverageValue
        averageValue: "5"  # Target 5 pending tasks per agent
  # Also consider CPU utilization
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  # And memory utilization
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 100  # Double the pods
        periodSeconds: 60
      - type: Pods
        value: 4    # Add max 4 pods
        periodSeconds: 60
      selectPolicy: Min  # Use the minimum of the two
    scaleDown:
      stabilizationWindowSeconds: 300  # 5 minutes
      policies:
      - type: Percent
        value: 10   # Remove 10% of pods
        periodSeconds: 60
      - type: Pods
        value: 2    # Remove max 2 pods
        periodSeconds: 60
      selectPolicy: Max  # Use the maximum of the two
---
# HPA for Code Generator Agents
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: code-generator-agent-hpa
  namespace: agent-orchestrator
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: code-generator-agent
  minReplicas: 3
  maxReplicas: 150
  metrics:
  - type: External
    external:
      metric:
        name: pending_tasks_per_agent
        selector:
          matchLabels:
            agent_type: "code_generator"
      target:
        type: AverageValue
        averageValue: "5"
  - type: External
    external:
      metric:
        name: agent_utilization_ratio
        selector:
          matchLabels:
            agent_type: "code_generator"
      target:
        type: Value
        value: "0.8"  # Target 80% utilization
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 75
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 30
      policies:
      - type: Percent
        value: 50
        periodSeconds: 30
      - type: Pods
        value: 10
        periodSeconds: 30
      selectPolicy: Max  # Aggressive scaling up
    scaleDown:
      stabilizationWindowSeconds: 600  # 10 minutes
      policies:
      - type: Percent
        value: 5
        periodSeconds: 120
---
# HPA for Tester Agents
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: tester-agent-hpa
  namespace: agent-orchestrator
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: tester-agent
  minReplicas: 2
  maxReplicas: 80
  metrics:
  - type: External
    external:
      metric:
        name: pending_tasks_per_agent
        selector:
          matchLabels:
            agent_type: "tester"
      target:
        type: AverageValue
        averageValue: "8"  # Testers can handle more queued tasks
  - type: External
    external:
      metric:
        name: task_queue_depth
        selector:
          matchLabels:
            priority: "NORMAL"
      target:
        type: AverageValue
        averageValue: "100"  # Scale when queue depth is high
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 120
      policies:
      - type: Pods
        value: 5
        periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 900  # 15 minutes
      policies:
      - type: Pods
        value: 1
        periodSeconds: 300
---
# HPA for Reviewer Agents
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: reviewer-agent-hpa
  namespace: agent-orchestrator
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: reviewer-agent
  minReplicas: 1
  maxReplicas: 50
  metrics:
  - type: External
    external:
      metric:
        name: pending_tasks_per_agent
        selector:
          matchLabels:
            agent_type: "reviewer"
      target:
        type: AverageValue
        averageValue: "3"  # Reviewers need more time per task
  - type: External
    external:
      metric:
        name: task_duration_seconds_p99
        selector:
          matchLabels:
            agent_type: "reviewer"
      target:
        type: Value
        value: "300"  # Scale if p99 latency > 5 minutes
---
# HPA for Doc Generator Agents
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: doc-generator-agent-hpa
  namespace: agent-orchestrator
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: doc-generator-agent
  minReplicas: 1
  maxReplicas: 30
  metrics:
  - type: External
    external:
      metric:
        name: pending_tasks_per_agent
        selector:
          matchLabels:
            agent_type: "doc_generator"
      target:
        type: AverageValue
        averageValue: "10"
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 180
    scaleDown:
      stabilizationWindowSeconds: 1800  # 30 minutes
---
# Prometheus Adapter configuration to expose custom metrics
apiVersion: v1
kind: ConfigMap
metadata:
  name: adapter-config
  namespace: monitoring
data:
  config.yaml: |
    rules:
    # Expose pending_tasks_per_agent metric
    - seriesQuery: 'pending_tasks_per_agent{namespace!="",pod!=""}'
      resources:
        overrides:
          namespace:
            resource: namespace
          pod:
            resource: pod
      name:
        matches: "^pending_tasks_per_agent"
        as: "pending_tasks_per_agent"
      metricsQuery: 'avg_over_time(pending_tasks_per_agent{<<.LabelMatchers>>}[5m])'
    
    # Expose agent_utilization_ratio metric
    - seriesQuery: 'agent_utilization_ratio{namespace!="",pod!=""}'
      resources:
        overrides:
          namespace:
            resource: namespace
          pod:
            resource: pod
      name:
        matches: "^agent_utilization_ratio"
        as: "agent_utilization_ratio"
      metricsQuery: 'avg_over_time(agent_utilization_ratio{<<.LabelMatchers>>}[2m])'
    
    # Expose task_queue_depth metric
    - seriesQuery: 'task_queue_depth'
      name:
        matches: "^task_queue_depth"
        as: "task_queue_depth"
      metricsQuery: 'max_over_time(task_queue_depth{<<.LabelMatchers>>}[1m])'
    
    # Expose task duration p99
    - seriesQuery: 'agent_task_duration_seconds'
      name:
        matches: "^agent_task_duration_seconds"
        as: "task_duration_seconds_p99"
      metricsQuery: 'histogram_quantile(0.99, rate(agent_task_duration_seconds_bucket{<<.LabelMatchers>>}[5m]))'
---
# VPA (Vertical Pod Autoscaler) for right-sizing agent pods
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: planner-agent-vpa
  namespace: agent-orchestrator
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: planner-agent
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
    - containerName: agent
      minAllowed:
        cpu: 100m
        memory: 256Mi
      maxAllowed:
        cpu: 2
        memory: 4Gi
---
# Cost-aware scaling with Karpenter provisioner
apiVersion: karpenter.sh/v1alpha5
kind: Provisioner
metadata:
  name: agent-orchestrator-provisioner
  namespace: agent-orchestrator
spec:
  # Taints to isolate agent workloads
  taints:
    - key: agent-orchestrator.io/agent
      value: "true"
      effect: NoSchedule
  
  # Requirements for node selection
  requirements:
    - key: karpenter.sh/capacity-type
      operator: In
      values: ["spot", "on-demand"]  # Prefer spot for cost optimization
    - key: kubernetes.io/arch
      operator: In
      values: ["amd64"]
    - key: node.kubernetes.io/instance-type
      operator: In
      values:
        # Cost-optimized instance types
        - t3.medium
        - t3.large
        - t3a.medium
        - t3a.large
        - m5.large
        - m5a.large
        - c5.large
        - c5a.large
  
  # Disruption settings
  ttlSecondsAfterEmpty: 300  # Remove empty nodes after 5 minutes
  
  # Resource limits
  limits:
    resources:
      cpu: 1000
      memory: 1000Gi
  
  # Node properties
  providerRef:
    name: default
  
  # User data for node initialization
  userData: |
    #!/bin/bash
    # Install monitoring agents
    curl -fsSL https://get.docker.com | sh
    systemctl enable docker
    
    # Configure sysctl for high-performance
    echo "net.core.rmem_max = 134217728" >> /etc/sysctl.conf
    echo "net.core.wmem_max = 134217728" >> /etc/sysctl.conf
    echo "net.ipv4.tcp_rmem = 4096 87380 134217728" >> /etc/sysctl.conf
    echo "net.ipv4.tcp_wmem = 4096 65536 134217728" >> /etc/sysctl.conf
    sysctl -p
---
# PodDisruptionBudget to ensure availability during scaling
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: planner-agent-pdb
  namespace: agent-orchestrator
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: planner-agent
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: code-generator-agent-pdb
  namespace: agent-orchestrator
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: code-generator-agent
---
# NetworkPolicy for security
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: agent-orchestrator-network-policy
  namespace: agent-orchestrator
spec:
  podSelector:
    matchLabels:
      component: agent
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: agent-orchestrator
    - podSelector:
        matchLabels:
          component: orchestrator
    ports:
    - protocol: TCP
      port: 50051  # gRPC port
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          name: agent-orchestrator
    ports:
    - protocol: TCP
      port: 5432   # PostgreSQL
    - protocol: TCP
      port: 6379   # Redis
    - protocol: TCP
      port: 50051  # gRPC
  - to:
    - namespaceSelector: {}
    ports:
    - protocol: TCP
      port: 443    # HTTPS for external APIs
---
# Priority classes for critical agents
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: critical-agent-priority
value: 1000
globalDefault: false
description: "Priority class for critical orchestrator agents"
---
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-agent-priority
value: 500
globalDefault: false
description: "Priority class for high-priority agents"
---
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: normal-agent-priority
value: 0
globalDefault: true
description: "Default priority class for agents"